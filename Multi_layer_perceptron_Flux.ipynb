{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: onehotbatch, onecold, @epochs\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using MLDatasets\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and prepare the data\n",
    "\n",
    "We load the MNIST train and test data using MLDatasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load full train set\n",
    "train_x, train_y = MLDatasets.MNIST.traindata(Float32)\n",
    "\n",
    "# load full test set\n",
    "test_x, test_y = MLDatasets.MNIST.testdata(Float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: (28, 28, 60000)\n",
      "Size of test set: (28, 28, 10000)\n"
     ]
    }
   ],
   "source": [
    "println(\"Size of train set: $(size(train_x))\")\n",
    "println(\"Size of test set: $(size(test_x))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the train dataset has 60000 examples and the test dataset has 10000. Each element of both datasets is a 28x28 matrix. Flux expects the data to be in a different shape . \n",
    "We need to transform the MNIST data so we can feed it into our Flux model. Thus, we flatten the input data, that is, we convert each 28x28 matrix into a 784-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784×10000 Array{Float32,2}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱            ⋮                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = Flux.flatten(train_x)\n",
    "test_x = Flux.flatten(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/code/machine_learning/preprocessing_structured_data/one-hot_encode_nominal_categorical_features/\n",
    "\n",
    "Now, we need to one-hot encode (see image above - Chris Albon) the labels so that our model can understand them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bool[0 1 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 1; 0 0 … 0 0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode the labels\n",
    "train_y, test_y = onehotbatch(train_y, 0:9), onehotbatch(test_y, 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we are working with a very large dataset, it is more convenient to work with mini batches of the data. We use Flux's [DataLoader](https://fluxml.ai/Flux.jl/stable/data/dataloader/) type so we can iterate over the mini batches of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoader{Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}}((Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0]), 256, 10000, true, 10000, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000], false)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders (mini-batch iterators)\n",
    "train_data_loader = DataLoader((train_x, train_y), batchsize=256, shuffle=true)\n",
    "test_data_loader = DataLoader((test_x, test_y), batchsize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "As we mentioned above, our model has one input layer, one hidden layer (with 32 perceptrons), and an output layer (with num_classes perceptrons). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.07582425 0.02542828 … -0.0016211248 -0.06204374; 0.0057818107 0.07168816 … 0.03780225 -0.040162228; … ; -0.026540529 0.07710746 … 0.06315562 0.015118687; 0.035058614 -0.012984065 … -0.054886896 -0.0048402925], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.09892084 -0.2296166 … 0.23926364 0.34173307; -0.029866401 -0.35210624 … 0.2280323 -0.13073415; … ; 0.16927409 0.36291304 … -0.16681334 0.012161752; -0.11990734 -0.30701593 … 0.3535769 0.27875617], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct model\n",
    "img_size = (28,28,1)\n",
    "num_classes = 10\n",
    "\n",
    "model = Chain( Dense(prod(img_size), 32, relu),\n",
    "                  Dense(32, num_classes))\n",
    "\n",
    "ps = Flux.params(model) # model's trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a loss function and an accuracy function. It is very important to monitor loss during training time so we can decide when it's best to stop the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(data_loader, model)\n",
    "    total_loss = 0.0f0\n",
    "    num_elements = 0\n",
    "    for (x, y) in data_loader\n",
    "        ŷ = model(x)\n",
    "        total_loss += logitcrossentropy(ŷ, y, agg=sum)\n",
    "        num_elements +=  size(x)[end]\n",
    "    end\n",
    "    return total_loss / num_elements\n",
    "end\n",
    "\n",
    "\n",
    "function accuracy(data_loader, model)\n",
    "    accuracy = 0\n",
    "    num_elements = 0\n",
    "    for (x, y) in data_loader\n",
    "        ŷ = model(x)\n",
    "        accuracy += sum(onecold(ŷ) .== onecold(y))\n",
    "        num_elements += size(x)[end]\n",
    "    end   \n",
    "    \n",
    "    return accuracy / num_elements\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Descent optimiser](https://fluxml.ai/Flux.jl/stable/training/optimisers/) (Gradient Descent) and set a value η for the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descent(0.0003)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "η = 3e-4 \n",
    "\n",
    "opt = Descent(η)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model. We train a neural network in stages (or epochs). In each epoch, we perform one step of the gradient descent algorithm and output the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1\n",
      "  train_loss = 2.2848907, train_accuracy = 0.11465\n",
      "  test_loss = 2.280792, test_accuracy = 0.1162\n",
      "Epoch=2\n",
      "  train_loss = 2.2361443, train_accuracy = 0.13995\n",
      "  test_loss = 2.2309375, test_accuracy = 0.1418\n",
      "Epoch=3\n",
      "  train_loss = 2.1969159, train_accuracy = 0.17005\n",
      "  test_loss = 2.1906798, test_accuracy = 0.1722\n",
      "Epoch=4\n",
      "  train_loss = 2.1618845, train_accuracy = 0.1993\n",
      "  test_loss = 2.1545837, test_accuracy = 0.2032\n",
      "Epoch=5\n",
      "  train_loss = 2.1286616, train_accuracy = 0.22865\n",
      "  test_loss = 2.1203725, test_accuracy = 0.2326\n",
      "Epoch=6\n",
      "  train_loss = 2.0960646, train_accuracy = 0.25888333333333335\n",
      "  test_loss = 2.0867333, test_accuracy = 0.2592\n",
      "Epoch=7\n",
      "  train_loss = 2.0636058, train_accuracy = 0.29101666666666665\n",
      "  test_loss = 2.05321, test_accuracy = 0.2915\n",
      "Epoch=8\n",
      "  train_loss = 2.0309577, train_accuracy = 0.32438333333333336\n",
      "  test_loss = 2.0194302, test_accuracy = 0.3269\n",
      "Epoch=9\n",
      "  train_loss = 1.9979835, train_accuracy = 0.35613333333333336\n",
      "  test_loss = 1.9853249, test_accuracy = 0.3601\n",
      "Epoch=10\n",
      "  train_loss = 1.9646773, train_accuracy = 0.38738333333333336\n",
      "  test_loss = 1.9509133, test_accuracy = 0.3936\n",
      "Epoch=11\n",
      "  train_loss = 1.9311335, train_accuracy = 0.41715\n",
      "  test_loss = 1.916232, test_accuracy = 0.4256\n",
      "Epoch=12\n",
      "  train_loss = 1.897462, train_accuracy = 0.44545\n",
      "  test_loss = 1.8814294, test_accuracy = 0.455\n",
      "Epoch=13\n",
      "  train_loss = 1.8638355, train_accuracy = 0.4719833333333333\n",
      "  test_loss = 1.846669, test_accuracy = 0.483\n",
      "Epoch=14\n",
      "  train_loss = 1.830326, train_accuracy = 0.49646666666666667\n",
      "  test_loss = 1.8120607, test_accuracy = 0.5079\n",
      "Epoch=15\n",
      "  train_loss = 1.7970588, train_accuracy = 0.5179666666666667\n",
      "  test_loss = 1.7777057, test_accuracy = 0.5297\n",
      "Epoch=16\n",
      "  train_loss = 1.7640983, train_accuracy = 0.5374\n",
      "  test_loss = 1.7437164, test_accuracy = 0.5494\n",
      "Epoch=17\n",
      "  train_loss = 1.7314767, train_accuracy = 0.5556333333333333\n",
      "  test_loss = 1.7101336, test_accuracy = 0.5688\n",
      "Epoch=18\n",
      "  train_loss = 1.6991946, train_accuracy = 0.5719666666666666\n",
      "  test_loss = 1.6769131, test_accuracy = 0.5849\n",
      "Epoch=19\n",
      "  train_loss = 1.6672748, train_accuracy = 0.5870166666666666\n",
      "  test_loss = 1.6441047, test_accuracy = 0.601\n",
      "Epoch=20\n",
      "  train_loss = 1.6357402, train_accuracy = 0.6005333333333334\n",
      "  test_loss = 1.6117631, test_accuracy = 0.6126\n",
      "Epoch=21\n",
      "  train_loss = 1.6046219, train_accuracy = 0.6125833333333334\n",
      "  test_loss = 1.5798833, test_accuracy = 0.6251\n",
      "Epoch=22\n",
      "  train_loss = 1.5739152, train_accuracy = 0.6246166666666667\n",
      "  test_loss = 1.5484598, test_accuracy = 0.6382\n",
      "Epoch=23\n",
      "  train_loss = 1.5436714, train_accuracy = 0.6355666666666666\n",
      "  test_loss = 1.5175312, test_accuracy = 0.6497\n",
      "Epoch=24\n",
      "  train_loss = 1.5138903, train_accuracy = 0.6462333333333333\n",
      "  test_loss = 1.4871159, test_accuracy = 0.6594\n",
      "Epoch=25\n",
      "  train_loss = 1.4845566, train_accuracy = 0.6559333333333334\n",
      "  test_loss = 1.4571977, test_accuracy = 0.6697\n",
      "Epoch=26\n",
      "  train_loss = 1.4557457, train_accuracy = 0.6650833333333334\n",
      "  test_loss = 1.4278291, test_accuracy = 0.6784\n",
      "Epoch=27\n",
      "  train_loss = 1.4274709, train_accuracy = 0.6739166666666667\n",
      "  test_loss = 1.3990427, test_accuracy = 0.6874\n",
      "Epoch=28\n",
      "  train_loss = 1.3997811, train_accuracy = 0.6822\n",
      "  test_loss = 1.3708577, test_accuracy = 0.6975\n",
      "Epoch=29\n",
      "  train_loss = 1.3727245, train_accuracy = 0.6901333333333334\n",
      "  test_loss = 1.3433299, test_accuracy = 0.7063\n",
      "Epoch=30\n",
      "  train_loss = 1.3463402, train_accuracy = 0.6984166666666667\n",
      "  test_loss = 1.316495, test_accuracy = 0.7136\n",
      "Epoch=31\n",
      "  train_loss = 1.3206127, train_accuracy = 0.7065\n",
      "  test_loss = 1.2903657, test_accuracy = 0.7204\n",
      "Epoch=32\n",
      "  train_loss = 1.2955655, train_accuracy = 0.7135166666666667\n",
      "  test_loss = 1.2649531, test_accuracy = 0.7281\n",
      "Epoch=33\n",
      "  train_loss = 1.2711998, train_accuracy = 0.7203166666666667\n",
      "  test_loss = 1.2402357, test_accuracy = 0.7335\n",
      "Epoch=34\n",
      "  train_loss = 1.2475418, train_accuracy = 0.7261166666666666\n",
      "  test_loss = 1.2162472, test_accuracy = 0.7375\n",
      "Epoch=35\n",
      "  train_loss = 1.2245891, train_accuracy = 0.7317666666666667\n",
      "  test_loss = 1.192996, test_accuracy = 0.745\n",
      "Epoch=36\n",
      "  train_loss = 1.2023602, train_accuracy = 0.7365833333333334\n",
      "  test_loss = 1.1704924, test_accuracy = 0.7507\n",
      "Epoch=37\n",
      "  train_loss = 1.1808485, train_accuracy = 0.7413666666666666\n",
      "  test_loss = 1.1487095, test_accuracy = 0.7563\n",
      "Epoch=38\n",
      "  train_loss = 1.1600444, train_accuracy = 0.7455166666666667\n",
      "  test_loss = 1.127678, test_accuracy = 0.759\n",
      "Epoch=39\n",
      "  train_loss = 1.1399477, train_accuracy = 0.7496833333333334\n",
      "  test_loss = 1.1074015, test_accuracy = 0.7633\n",
      "Epoch=40\n",
      "  train_loss = 1.1205481, train_accuracy = 0.75445\n",
      "  test_loss = 1.0878605, test_accuracy = 0.7677\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "train_accuracy_results = []\n",
    "test_accuracy_results = []\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    for (x, y) in train_data_loader\n",
    "        gs = gradient(() -> logitcrossentropy(model(x), y), ps) # Compute gradient\n",
    "        Flux.Optimise.update!(opt, ps, gs) # Update parameters\n",
    "     end\n",
    "        \n",
    "    # Compute accuracy and loss for all of the train and test data\n",
    "    train_loss = loss(train_data_loader, model)\n",
    "    train_acc = accuracy(train_data_loader, model)\n",
    "    test_loss = loss(test_data_loader, model)\n",
    "    test_acc = accuracy(test_data_loader, model)\n",
    "    println(\"Epoch=$epoch\")\n",
    "    println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "    println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "    push!(train_loss_results, train_loss)\n",
    "    push!(test_loss_results, test_loss)\n",
    "    push!(train_accuracy_results, train_acc)\n",
    "    push!(test_accuracy_results, test_acc)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train and test loss to check overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip2200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2200)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2201\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2200)\" d=\"\n",
       "M172.918 1486.45 L2352.76 1486.45 L2352.76 123.472 L172.918 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2202\">\n",
       "    <rect x=\"172\" y=\"123\" width=\"2181\" height=\"1364\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  181.882,1486.45 181.882,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  709.177,1486.45 709.177,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1236.47,1486.45 1236.47,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1763.77,1486.45 1763.77,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2291.06,1486.45 2291.06,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.918,1273.71 2352.76,1273.71 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.918,1005.16 2352.76,1005.16 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.918,736.616 2352.76,736.616 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.918,468.071 2352.76,468.071 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.918,468.071 2352.76,468.071 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,1486.45 172.918,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  181.882,1486.45 181.882,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  709.177,1486.45 709.177,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1236.47,1486.45 1236.47,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1763.77,1486.45 1763.77,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2291.06,1486.45 2291.06,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,1273.71 199.076,1273.71 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,1005.16 199.076,1005.16 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,736.616 199.076,736.616 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,468.071 199.076,468.071 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.918,468.071 199.076,468.071 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2200)\" d=\"M 0 0 M181.882 1508.44 Q178.271 1508.44 176.442 1512 Q174.636 1515.55 174.636 1522.67 Q174.636 1529.78 176.442 1533.35 Q178.271 1536.89 181.882 1536.89 Q185.516 1536.89 187.321 1533.35 Q189.15 1529.78 189.15 1522.67 Q189.15 1515.55 187.321 1512 Q185.516 1508.44 181.882 1508.44 M181.882 1504.73 Q187.692 1504.73 190.747 1509.34 Q193.826 1513.92 193.826 1522.67 Q193.826 1531.4 190.747 1536.01 Q187.692 1540.59 181.882 1540.59 Q176.071 1540.59 172.993 1536.01 Q169.937 1531.4 169.937 1522.67 Q169.937 1513.92 172.993 1509.34 Q176.071 1504.73 181.882 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M686.052 1535.98 L693.691 1535.98 L693.691 1509.62 L685.381 1511.29 L685.381 1507.03 L693.644 1505.36 L698.32 1505.36 L698.32 1535.98 L705.959 1535.98 L705.959 1539.92 L686.052 1539.92 L686.052 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M721.029 1508.44 Q717.417 1508.44 715.589 1512 Q713.783 1515.55 713.783 1522.67 Q713.783 1529.78 715.589 1533.35 Q717.417 1536.89 721.029 1536.89 Q724.663 1536.89 726.468 1533.35 Q728.297 1529.78 728.297 1522.67 Q728.297 1515.55 726.468 1512 Q724.663 1508.44 721.029 1508.44 M721.029 1504.73 Q726.839 1504.73 729.894 1509.34 Q732.973 1513.92 732.973 1522.67 Q732.973 1531.4 729.894 1536.01 Q726.839 1540.59 721.029 1540.59 Q715.218 1540.59 712.14 1536.01 Q709.084 1531.4 709.084 1522.67 Q709.084 1513.92 712.14 1509.34 Q715.218 1504.73 721.029 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1217.62 1535.98 L1233.94 1535.98 L1233.94 1539.92 L1211.99 1539.92 L1211.99 1535.98 Q1214.65 1533.23 1219.24 1528.6 Q1223.84 1523.95 1225.03 1522.61 Q1227.27 1520.08 1228.15 1518.35 Q1229.05 1516.59 1229.05 1514.9 Q1229.05 1512.14 1227.11 1510.41 Q1225.19 1508.67 1222.09 1508.67 Q1219.89 1508.67 1217.43 1509.43 Q1215 1510.2 1212.22 1511.75 L1212.22 1507.03 Q1215.05 1505.89 1217.5 1505.31 Q1219.96 1504.73 1221.99 1504.73 Q1227.36 1504.73 1230.56 1507.42 Q1233.75 1510.11 1233.75 1514.6 Q1233.75 1516.73 1232.94 1518.65 Q1232.15 1520.54 1230.05 1523.14 Q1229.47 1523.81 1226.37 1527.03 Q1223.27 1530.22 1217.62 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1249.01 1508.44 Q1245.4 1508.44 1243.57 1512 Q1241.76 1515.55 1241.76 1522.67 Q1241.76 1529.78 1243.57 1533.35 Q1245.4 1536.89 1249.01 1536.89 Q1252.64 1536.89 1254.45 1533.35 Q1256.28 1529.78 1256.28 1522.67 Q1256.28 1515.55 1254.45 1512 Q1252.64 1508.44 1249.01 1508.44 M1249.01 1504.73 Q1254.82 1504.73 1257.87 1509.34 Q1260.95 1513.92 1260.95 1522.67 Q1260.95 1531.4 1257.87 1536.01 Q1254.82 1540.59 1249.01 1540.59 Q1243.2 1540.59 1240.12 1536.01 Q1237.06 1531.4 1237.06 1522.67 Q1237.06 1513.92 1240.12 1509.34 Q1243.2 1504.73 1249.01 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1754.51 1521.29 Q1757.86 1522 1759.74 1524.27 Q1761.64 1526.54 1761.64 1529.87 Q1761.64 1534.99 1758.12 1537.79 Q1754.6 1540.59 1748.12 1540.59 Q1745.94 1540.59 1743.63 1540.15 Q1741.34 1539.73 1738.88 1538.88 L1738.88 1534.36 Q1740.83 1535.5 1743.14 1536.08 Q1745.46 1536.66 1747.98 1536.66 Q1752.38 1536.66 1754.67 1534.92 Q1756.98 1533.18 1756.98 1529.87 Q1756.98 1526.82 1754.83 1525.11 Q1752.7 1523.37 1748.88 1523.37 L1744.86 1523.37 L1744.86 1519.53 L1749.07 1519.53 Q1752.52 1519.53 1754.35 1518.16 Q1756.17 1516.77 1756.17 1514.18 Q1756.17 1511.52 1754.28 1510.11 Q1752.4 1508.67 1748.88 1508.67 Q1746.96 1508.67 1744.76 1509.09 Q1742.56 1509.5 1739.92 1510.38 L1739.92 1506.22 Q1742.59 1505.48 1744.9 1505.11 Q1747.24 1504.73 1749.3 1504.73 Q1754.62 1504.73 1757.73 1507.17 Q1760.83 1509.57 1760.83 1513.69 Q1760.83 1516.56 1759.18 1518.55 Q1757.54 1520.52 1754.51 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1776.71 1508.44 Q1773.1 1508.44 1771.27 1512 Q1769.46 1515.55 1769.46 1522.67 Q1769.46 1529.78 1771.27 1533.35 Q1773.1 1536.89 1776.71 1536.89 Q1780.34 1536.89 1782.15 1533.35 Q1783.98 1529.78 1783.98 1522.67 Q1783.98 1515.55 1782.15 1512 Q1780.34 1508.44 1776.71 1508.44 M1776.71 1504.73 Q1782.52 1504.73 1785.57 1509.34 Q1788.65 1513.92 1788.65 1522.67 Q1788.65 1531.4 1785.57 1536.01 Q1782.52 1540.59 1776.71 1540.59 Q1770.9 1540.59 1767.82 1536.01 Q1764.76 1531.4 1764.76 1522.67 Q1764.76 1513.92 1767.82 1509.34 Q1770.9 1504.73 1776.71 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2280.56 1509.43 L2268.76 1527.88 L2280.56 1527.88 L2280.56 1509.43 M2279.34 1505.36 L2285.22 1505.36 L2285.22 1527.88 L2290.15 1527.88 L2290.15 1531.77 L2285.22 1531.77 L2285.22 1539.92 L2280.56 1539.92 L2280.56 1531.77 L2264.96 1531.77 L2264.96 1527.26 L2279.34 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2305.22 1508.44 Q2301.61 1508.44 2299.78 1512 Q2297.97 1515.55 2297.97 1522.67 Q2297.97 1529.78 2299.78 1533.35 Q2301.61 1536.89 2305.22 1536.89 Q2308.85 1536.89 2310.66 1533.35 Q2312.49 1529.78 2312.49 1522.67 Q2312.49 1515.55 2310.66 1512 Q2308.85 1508.44 2305.22 1508.44 M2305.22 1504.73 Q2311.03 1504.73 2314.08 1509.34 Q2317.16 1513.92 2317.16 1522.67 Q2317.16 1531.4 2314.08 1536.01 Q2311.03 1540.59 2305.22 1540.59 Q2299.41 1540.59 2296.33 1536.01 Q2293.27 1531.4 2293.27 1522.67 Q2293.27 1513.92 2296.33 1509.34 Q2299.41 1504.73 2305.22 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M67.6217 1287.05 L75.2606 1287.05 L75.2606 1260.69 L66.9504 1262.35 L66.9504 1258.09 L75.2143 1256.43 L79.8902 1256.43 L79.8902 1287.05 L87.529 1287.05 L87.529 1290.99 L67.6217 1290.99 L67.6217 1287.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M92.5984 1285.11 L97.4827 1285.11 L97.4827 1290.99 L92.5984 1290.99 L92.5984 1285.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M106.58 1287.05 L122.899 1287.05 L122.899 1290.99 L100.955 1290.99 L100.955 1287.05 Q103.617 1284.3 108.2 1279.67 Q112.807 1275.01 113.987 1273.67 Q116.233 1271.15 117.112 1269.41 Q118.015 1267.65 118.015 1265.96 Q118.015 1263.21 116.071 1261.47 Q114.149 1259.74 111.047 1259.74 Q108.848 1259.74 106.395 1260.5 Q103.964 1261.26 101.186 1262.81 L101.186 1258.09 Q104.01 1256.96 106.464 1256.38 Q108.918 1255.8 110.955 1255.8 Q116.325 1255.8 119.52 1258.49 Q122.714 1261.17 122.714 1265.66 Q122.714 1267.79 121.904 1269.71 Q121.117 1271.61 119.01 1274.2 Q118.432 1274.87 115.33 1278.09 Q112.228 1281.29 106.58 1287.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M128.015 1256.43 L146.371 1256.43 L146.371 1260.36 L132.297 1260.36 L132.297 1268.83 Q133.316 1268.49 134.334 1268.32 Q135.353 1268.14 136.371 1268.14 Q142.158 1268.14 145.538 1271.31 Q148.918 1274.48 148.918 1279.9 Q148.918 1285.48 145.445 1288.58 Q141.973 1291.66 135.654 1291.66 Q133.478 1291.66 131.209 1291.29 Q128.964 1290.92 126.557 1290.18 L126.557 1285.48 Q128.64 1286.61 130.862 1287.17 Q133.084 1287.72 135.561 1287.72 Q139.566 1287.72 141.904 1285.62 Q144.242 1283.51 144.242 1279.9 Q144.242 1276.29 141.904 1274.18 Q139.566 1272.07 135.561 1272.07 Q133.686 1272.07 131.811 1272.49 Q129.959 1272.91 128.015 1273.79 L128.015 1256.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M66.0245 1018.51 L73.6634 1018.51 L73.6634 992.14 L65.3532 993.807 L65.3532 989.548 L73.6171 987.881 L78.293 987.881 L78.293 1018.51 L85.9318 1018.51 L85.9318 1022.44 L66.0245 1022.44 L66.0245 1018.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M91.0012 1016.56 L95.8855 1016.56 L95.8855 1022.44 L91.0012 1022.44 L91.0012 1016.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M101.001 987.881 L119.358 987.881 L119.358 991.816 L105.284 991.816 L105.284 1000.29 Q106.302 999.941 107.321 999.779 Q108.339 999.594 109.358 999.594 Q115.145 999.594 118.524 1002.77 Q121.904 1005.94 121.904 1011.35 Q121.904 1016.93 118.432 1020.03 Q114.959 1023.11 108.64 1023.11 Q106.464 1023.11 104.196 1022.74 Q101.95 1022.37 99.5429 1021.63 L99.5429 1016.93 Q101.626 1018.07 103.848 1018.62 Q106.071 1019.18 108.547 1019.18 Q112.552 1019.18 114.89 1017.07 Q117.228 1014.96 117.228 1011.35 Q117.228 1007.74 114.89 1005.64 Q112.552 1003.53 108.547 1003.53 Q106.672 1003.53 104.797 1003.95 Q102.946 1004.36 101.001 1005.24 L101.001 987.881 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M136.973 990.96 Q133.362 990.96 131.533 994.524 Q129.728 998.066 129.728 1005.2 Q129.728 1012.3 131.533 1015.87 Q133.362 1019.41 136.973 1019.41 Q140.607 1019.41 142.413 1015.87 Q144.242 1012.3 144.242 1005.2 Q144.242 998.066 142.413 994.524 Q140.607 990.96 136.973 990.96 M136.973 987.256 Q142.783 987.256 145.839 991.862 Q148.918 996.446 148.918 1005.2 Q148.918 1013.92 145.839 1018.53 Q142.783 1023.11 136.973 1023.11 Q131.163 1023.11 128.084 1018.53 Q125.029 1013.92 125.029 1005.2 Q125.029 996.446 128.084 991.862 Q131.163 987.256 136.973 987.256 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M66.9273 749.961 L74.5661 749.961 L74.5661 723.595 L66.256 725.262 L66.256 721.003 L74.5198 719.336 L79.1957 719.336 L79.1957 749.961 L86.8346 749.961 L86.8346 753.896 L66.9273 753.896 L66.9273 749.961 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M91.904 748.016 L96.7882 748.016 L96.7882 753.896 L91.904 753.896 L91.904 748.016 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M100.677 719.336 L122.899 719.336 L122.899 721.327 L110.353 753.896 L105.469 753.896 L117.274 723.271 L100.677 723.271 L100.677 719.336 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M128.015 719.336 L146.371 719.336 L146.371 723.271 L132.297 723.271 L132.297 731.743 Q133.316 731.396 134.334 731.234 Q135.353 731.049 136.371 731.049 Q142.158 731.049 145.538 734.22 Q148.918 737.391 148.918 742.808 Q148.918 748.387 145.445 751.488 Q141.973 754.567 135.654 754.567 Q133.478 754.567 131.209 754.197 Q128.964 753.826 126.557 753.086 L126.557 748.387 Q128.64 749.521 130.862 750.076 Q133.084 750.632 135.561 750.632 Q139.566 750.632 141.904 748.526 Q144.242 746.419 144.242 742.808 Q144.242 739.197 141.904 737.09 Q139.566 734.984 135.561 734.984 Q133.686 734.984 131.811 735.401 Q129.959 735.817 128.015 736.697 L128.015 719.336 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M68.6171 481.416 L84.9365 481.416 L84.9365 485.351 L62.9921 485.351 L62.9921 481.416 Q65.6541 478.661 70.2375 474.031 Q74.8439 469.379 76.0245 468.036 Q78.2698 465.513 79.1494 463.777 Q80.0522 462.018 80.0522 460.328 Q80.0522 457.573 78.1078 455.837 Q76.1865 454.101 73.0847 454.101 Q70.8856 454.101 68.4319 454.865 Q66.0014 455.629 63.2236 457.18 L63.2236 452.457 Q66.0477 451.323 68.5014 450.745 Q70.955 450.166 72.9921 450.166 Q78.3624 450.166 81.5568 452.851 Q84.7513 455.536 84.7513 460.027 Q84.7513 462.157 83.9411 464.078 Q83.1541 465.976 81.0476 468.569 Q80.4689 469.24 77.367 472.457 Q74.2652 475.652 68.6171 481.416 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M90.0059 479.471 L94.8901 479.471 L94.8901 485.351 L90.0059 485.351 L90.0059 479.471 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M109.959 453.87 Q106.348 453.87 104.52 457.434 Q102.714 460.976 102.714 468.106 Q102.714 475.212 104.52 478.777 Q106.348 482.318 109.959 482.318 Q113.594 482.318 115.399 478.777 Q117.228 475.212 117.228 468.106 Q117.228 460.976 115.399 457.434 Q113.594 453.87 109.959 453.87 M109.959 450.166 Q115.77 450.166 118.825 454.772 Q121.904 459.356 121.904 468.106 Q121.904 476.832 118.825 481.439 Q115.77 486.022 109.959 486.022 Q104.149 486.022 101.071 481.439 Q98.0151 476.832 98.0151 468.106 Q98.0151 459.356 101.071 454.772 Q104.149 450.166 109.959 450.166 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M136.973 453.87 Q133.362 453.87 131.533 457.434 Q129.728 460.976 129.728 468.106 Q129.728 475.212 131.533 478.777 Q133.362 482.318 136.973 482.318 Q140.607 482.318 142.413 478.777 Q144.242 475.212 144.242 468.106 Q144.242 460.976 142.413 457.434 Q140.607 453.87 136.973 453.87 M136.973 450.166 Q142.783 450.166 145.839 454.772 Q148.918 459.356 148.918 468.106 Q148.918 476.832 145.839 481.439 Q142.783 486.022 136.973 486.022 Q131.163 486.022 128.084 481.439 Q125.029 476.832 125.029 468.106 Q125.029 459.356 128.084 454.772 Q131.163 450.166 136.973 450.166 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M68.6171 481.416 L84.9365 481.416 L84.9365 485.351 L62.9921 485.351 L62.9921 481.416 Q65.6541 478.661 70.2375 474.031 Q74.8439 469.379 76.0245 468.036 Q78.2698 465.513 79.1494 463.777 Q80.0522 462.018 80.0522 460.328 Q80.0522 457.573 78.1078 455.837 Q76.1865 454.101 73.0847 454.101 Q70.8856 454.101 68.4319 454.865 Q66.0014 455.629 63.2236 457.18 L63.2236 452.457 Q66.0477 451.323 68.5014 450.745 Q70.955 450.166 72.9921 450.166 Q78.3624 450.166 81.5568 452.851 Q84.7513 455.536 84.7513 460.027 Q84.7513 462.157 83.9411 464.078 Q83.1541 465.976 81.0476 468.569 Q80.4689 469.24 77.367 472.457 Q74.2652 475.652 68.6171 481.416 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M90.0059 479.471 L94.8901 479.471 L94.8901 485.351 L90.0059 485.351 L90.0059 479.471 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M109.959 453.87 Q106.348 453.87 104.52 457.434 Q102.714 460.976 102.714 468.106 Q102.714 475.212 104.52 478.777 Q106.348 482.318 109.959 482.318 Q113.594 482.318 115.399 478.777 Q117.228 475.212 117.228 468.106 Q117.228 460.976 115.399 457.434 Q113.594 453.87 109.959 453.87 M109.959 450.166 Q115.77 450.166 118.825 454.772 Q121.904 459.356 121.904 468.106 Q121.904 476.832 118.825 481.439 Q115.77 486.022 109.959 486.022 Q104.149 486.022 101.071 481.439 Q98.0151 476.832 98.0151 468.106 Q98.0151 459.356 101.071 454.772 Q104.149 450.166 109.959 450.166 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M136.973 453.87 Q133.362 453.87 131.533 457.434 Q129.728 460.976 129.728 468.106 Q129.728 475.212 131.533 478.777 Q133.362 482.318 136.973 482.318 Q140.607 482.318 142.413 478.777 Q144.242 475.212 144.242 468.106 Q144.242 460.976 142.413 457.434 Q140.607 453.87 136.973 453.87 M136.973 450.166 Q142.783 450.166 145.839 454.772 Q148.918 459.356 148.918 468.106 Q148.918 476.832 145.839 481.439 Q142.783 486.022 136.973 486.022 Q131.163 486.022 128.084 481.439 Q125.029 476.832 125.029 468.106 Q125.029 459.356 128.084 454.772 Q131.163 450.166 136.973 450.166 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1182.47 12.096 L1190.65 12.096 L1190.65 65.6895 L1220.1 65.6895 L1220.1 72.576 L1182.47 72.576 L1182.47 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1244.04 32.4315 Q1238.05 32.4315 1234.56 37.1306 Q1231.08 41.7891 1231.08 49.9314 Q1231.08 58.0738 1234.52 62.7728 Q1238 67.4314 1244.04 67.4314 Q1250 67.4314 1253.48 62.7323 Q1256.96 58.0333 1256.96 49.9314 Q1256.96 41.8701 1253.48 37.1711 Q1250 32.4315 1244.04 32.4315 M1244.04 26.1121 Q1253.76 26.1121 1259.31 32.4315 Q1264.86 38.7509 1264.86 49.9314 Q1264.86 61.0714 1259.31 67.4314 Q1253.76 73.7508 1244.04 73.7508 Q1234.28 73.7508 1228.73 67.4314 Q1223.22 61.0714 1223.22 49.9314 Q1223.22 38.7509 1228.73 32.4315 Q1234.28 26.1121 1244.04 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1301.6 28.5427 L1301.6 35.5912 Q1298.44 33.9709 1295.04 33.1607 Q1291.64 32.3505 1287.99 32.3505 Q1282.44 32.3505 1279.65 34.0519 Q1276.89 35.7533 1276.89 39.156 Q1276.89 41.7486 1278.88 43.2475 Q1280.86 44.7058 1286.86 46.0426 L1289.41 46.6097 Q1297.35 48.3111 1300.67 51.4303 Q1304.03 54.509 1304.03 60.0587 Q1304.03 66.3781 1299.01 70.0644 Q1294.03 73.7508 1285.28 73.7508 Q1281.63 73.7508 1277.66 73.0216 Q1273.73 72.3329 1269.36 70.9151 L1269.36 63.2184 Q1273.49 65.3654 1277.5 66.4591 Q1281.51 67.5124 1285.44 67.5124 Q1290.71 67.5124 1293.54 65.73 Q1296.38 63.9071 1296.38 60.6258 Q1296.38 57.5877 1294.31 55.9673 Q1292.29 54.3469 1285.36 52.8481 L1282.77 52.2405 Q1275.84 50.7821 1272.76 47.7845 Q1269.68 44.7463 1269.68 39.4801 Q1269.68 33.0797 1274.22 29.5959 Q1278.76 26.1121 1287.1 26.1121 Q1291.23 26.1121 1294.88 26.7198 Q1298.53 27.3274 1301.6 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M1340.78 28.5427 L1340.78 35.5912 Q1337.62 33.9709 1334.21 33.1607 Q1330.81 32.3505 1327.17 32.3505 Q1321.62 32.3505 1318.82 34.0519 Q1316.07 35.7533 1316.07 39.156 Q1316.07 41.7486 1318.05 43.2475 Q1320.04 44.7058 1326.03 46.0426 L1328.58 46.6097 Q1336.52 48.3111 1339.84 51.4303 Q1343.21 54.509 1343.21 60.0587 Q1343.21 66.3781 1338.18 70.0644 Q1333.2 73.7508 1324.45 73.7508 Q1320.81 73.7508 1316.84 73.0216 Q1312.91 72.3329 1308.53 70.9151 L1308.53 63.2184 Q1312.66 65.3654 1316.67 66.4591 Q1320.68 67.5124 1324.61 67.5124 Q1329.88 67.5124 1332.71 65.73 Q1335.55 63.9071 1335.55 60.6258 Q1335.55 57.5877 1333.48 55.9673 Q1331.46 54.3469 1324.53 52.8481 L1321.94 52.2405 Q1315.01 50.7821 1311.93 47.7845 Q1308.86 44.7463 1308.86 39.4801 Q1308.86 33.0797 1313.39 29.5959 Q1317.93 26.1121 1326.27 26.1121 Q1330.41 26.1121 1334.05 26.7198 Q1337.7 27.3274 1340.78 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip2202)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  234.611,162.047 287.341,214.409 340.07,256.548 392.8,294.178 445.529,329.865 498.259,364.88 550.988,399.747 603.718,434.817 656.447,470.237 709.177,506.014 \n",
       "  761.906,542.046 814.636,578.215 867.365,614.336 920.095,650.331 972.824,686.066 1025.55,721.472 1078.28,756.513 1131.01,791.19 1183.74,825.478 1236.47,859.352 \n",
       "  1289.2,892.778 1341.93,925.763 1394.66,958.25 1447.39,990.24 1500.12,1021.75 1552.85,1052.7 1605.58,1083.07 1658.31,1112.81 1711.04,1141.88 1763.77,1170.22 \n",
       "  1816.5,1197.86 1869.23,1224.76 1921.96,1250.93 1974.69,1276.35 2027.41,1301 2080.14,1324.88 2132.87,1347.99 2185.6,1370.33 2238.33,1391.92 2291.06,1412.76 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2202)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  234.611,166.45 287.341,220.002 340.07,263.246 392.8,302.02 445.529,338.769 498.259,374.904 550.988,410.914 603.718,447.199 656.447,483.835 709.177,520.799 \n",
       "  761.906,558.053 814.636,595.437 867.365,632.776 920.095,669.951 972.824,706.855 1025.55,743.366 1078.28,779.44 1131.01,815.124 1183.74,850.367 1236.47,885.107 \n",
       "  1289.2,919.352 1341.93,953.106 1394.66,986.329 1447.39,1019 1500.12,1051.14 1552.85,1082.69 1605.58,1113.61 1658.31,1143.88 1711.04,1173.45 1763.77,1202.28 \n",
       "  1816.5,1230.35 1869.23,1257.64 1921.96,1284.19 1974.69,1309.96 2027.41,1334.94 2080.14,1359.11 2132.87,1382.51 2185.6,1405.1 2238.33,1426.88 2291.06,1447.87 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2200)\" d=\"\n",
       "M1844.53 388.432 L2280.76 388.432 L2280.76 206.992 L1844.53 206.992  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1844.53,388.432 2280.76,388.432 2280.76,206.992 1844.53,206.992 1844.53,388.432 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2200)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1868.53,267.472 2012.53,267.472 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2036.53 250.192 L2065.77 250.192 L2065.77 254.127 L2053.5 254.127 L2053.5 284.752 L2048.8 284.752 L2048.8 254.127 L2036.53 254.127 L2036.53 250.192 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2078.29 262.808 Q2077.57 262.391 2076.72 262.206 Q2075.88 261.998 2074.86 261.998 Q2071.25 261.998 2069.31 264.359 Q2067.39 266.697 2067.39 271.095 L2067.39 284.752 L2063.1 284.752 L2063.1 258.826 L2067.39 258.826 L2067.39 262.854 Q2068.73 260.493 2070.88 259.359 Q2073.03 258.201 2076.11 258.201 Q2076.55 258.201 2077.09 258.271 Q2077.62 258.317 2078.27 258.433 L2078.29 262.808 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2094.54 271.72 Q2089.38 271.72 2087.39 272.9 Q2085.4 274.081 2085.4 276.928 Q2085.4 279.197 2086.88 280.539 Q2088.38 281.859 2090.95 281.859 Q2094.49 281.859 2096.62 279.359 Q2098.78 276.836 2098.78 272.669 L2098.78 271.72 L2094.54 271.72 M2103.03 269.961 L2103.03 284.752 L2098.78 284.752 L2098.78 280.817 Q2097.32 283.178 2095.14 284.312 Q2092.96 285.423 2089.82 285.423 Q2085.84 285.423 2083.47 283.201 Q2081.14 280.956 2081.14 277.206 Q2081.14 272.831 2084.05 270.609 Q2086.99 268.386 2092.8 268.386 L2098.78 268.386 L2098.78 267.97 Q2098.78 265.03 2096.83 263.433 Q2094.91 261.812 2091.41 261.812 Q2089.19 261.812 2087.09 262.345 Q2084.98 262.877 2083.03 263.942 L2083.03 260.007 Q2085.37 259.104 2087.57 258.664 Q2089.77 258.201 2091.85 258.201 Q2097.48 258.201 2100.26 261.118 Q2103.03 264.035 2103.03 269.961 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2107.5 258.826 L2111.76 258.826 L2111.76 284.752 L2107.5 284.752 L2107.5 258.826 M2107.5 248.734 L2111.76 248.734 L2111.76 254.127 L2107.5 254.127 L2107.5 248.734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2137.78 269.104 L2137.78 284.752 L2133.52 284.752 L2133.52 269.243 Q2133.52 265.562 2132.09 263.734 Q2130.65 261.905 2127.78 261.905 Q2124.33 261.905 2122.34 264.104 Q2120.35 266.303 2120.35 270.099 L2120.35 284.752 L2116.07 284.752 L2116.07 258.826 L2120.35 258.826 L2120.35 262.854 Q2121.88 260.516 2123.94 259.359 Q2126.02 258.201 2128.73 258.201 Q2133.2 258.201 2135.49 260.979 Q2137.78 263.734 2137.78 269.104 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2157.32 248.734 L2161.58 248.734 L2161.58 284.752 L2157.32 284.752 L2157.32 248.734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2176.09 261.812 Q2172.66 261.812 2170.67 264.498 Q2168.68 267.16 2168.68 271.812 Q2168.68 276.465 2170.65 279.15 Q2172.64 281.812 2176.09 281.812 Q2179.49 281.812 2181.48 279.127 Q2183.47 276.442 2183.47 271.812 Q2183.47 267.206 2181.48 264.521 Q2179.49 261.812 2176.09 261.812 M2176.09 258.201 Q2181.65 258.201 2184.82 261.812 Q2187.99 265.424 2187.99 271.812 Q2187.99 278.178 2184.82 281.812 Q2181.65 285.423 2176.09 285.423 Q2170.51 285.423 2167.34 281.812 Q2164.19 278.178 2164.19 271.812 Q2164.19 265.424 2167.34 261.812 Q2170.51 258.201 2176.09 258.201 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2208.98 259.59 L2208.98 263.618 Q2207.18 262.692 2205.23 262.229 Q2203.29 261.766 2201.21 261.766 Q2198.03 261.766 2196.44 262.738 Q2194.86 263.711 2194.86 265.655 Q2194.86 267.136 2196 267.993 Q2197.13 268.826 2200.56 269.59 L2202.02 269.914 Q2206.55 270.886 2208.45 272.669 Q2210.37 274.428 2210.37 277.599 Q2210.37 281.21 2207.5 283.317 Q2204.65 285.423 2199.65 285.423 Q2197.57 285.423 2195.3 285.007 Q2193.06 284.613 2190.56 283.803 L2190.56 279.405 Q2192.92 280.632 2195.21 281.257 Q2197.5 281.859 2199.75 281.859 Q2202.76 281.859 2204.38 280.84 Q2206 279.798 2206 277.923 Q2206 276.187 2204.82 275.261 Q2203.66 274.336 2199.7 273.479 L2198.22 273.132 Q2194.26 272.298 2192.5 270.586 Q2190.74 268.849 2190.74 265.84 Q2190.74 262.183 2193.33 260.192 Q2195.93 258.201 2200.7 258.201 Q2203.06 258.201 2205.14 258.549 Q2207.22 258.896 2208.98 259.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2231.37 259.59 L2231.37 263.618 Q2229.56 262.692 2227.62 262.229 Q2225.67 261.766 2223.59 261.766 Q2220.42 261.766 2218.82 262.738 Q2217.25 263.711 2217.25 265.655 Q2217.25 267.136 2218.38 267.993 Q2219.52 268.826 2222.94 269.59 L2224.4 269.914 Q2228.94 270.886 2230.83 272.669 Q2232.76 274.428 2232.76 277.599 Q2232.76 281.21 2229.89 283.317 Q2227.04 285.423 2222.04 285.423 Q2219.96 285.423 2217.69 285.007 Q2215.44 284.613 2212.94 283.803 L2212.94 279.405 Q2215.3 280.632 2217.59 281.257 Q2219.89 281.859 2222.13 281.859 Q2225.14 281.859 2226.76 280.84 Q2228.38 279.798 2228.38 277.923 Q2228.38 276.187 2227.2 275.261 Q2226.04 274.336 2222.08 273.479 L2220.6 273.132 Q2216.64 272.298 2214.89 270.586 Q2213.13 268.849 2213.13 265.84 Q2213.13 262.183 2215.72 260.192 Q2218.31 258.201 2223.08 258.201 Q2225.44 258.201 2227.52 258.549 Q2229.61 258.896 2231.37 259.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip2200)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1868.53,327.952 2012.53,327.952 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2036.53 310.672 L2065.77 310.672 L2065.77 314.607 L2053.5 314.607 L2053.5 345.232 L2048.8 345.232 L2048.8 314.607 L2036.53 314.607 L2036.53 310.672 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2084.35 331.204 L2084.35 333.288 L2064.77 333.288 Q2065.05 337.686 2067.41 340.001 Q2069.79 342.292 2074.03 342.292 Q2076.48 342.292 2078.78 341.69 Q2081.09 341.089 2083.36 339.885 L2083.36 343.913 Q2081.07 344.885 2078.66 345.394 Q2076.25 345.903 2073.78 345.903 Q2067.57 345.903 2063.94 342.292 Q2060.33 338.681 2060.33 332.524 Q2060.33 326.158 2063.75 322.431 Q2067.2 318.681 2073.03 318.681 Q2078.27 318.681 2081.3 322.061 Q2084.35 325.417 2084.35 331.204 M2080.09 329.954 Q2080.05 326.459 2078.13 324.376 Q2076.23 322.292 2073.08 322.292 Q2069.52 322.292 2067.36 324.306 Q2065.23 326.32 2064.91 329.978 L2080.09 329.954 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2105.35 320.07 L2105.35 324.098 Q2103.54 323.172 2101.6 322.709 Q2099.65 322.246 2097.57 322.246 Q2094.4 322.246 2092.8 323.218 Q2091.23 324.191 2091.23 326.135 Q2091.23 327.616 2092.36 328.473 Q2093.5 329.306 2096.92 330.07 L2098.38 330.394 Q2102.92 331.366 2104.82 333.149 Q2106.74 334.908 2106.74 338.079 Q2106.74 341.69 2103.87 343.797 Q2101.02 345.903 2096.02 345.903 Q2093.94 345.903 2091.67 345.487 Q2089.42 345.093 2086.92 344.283 L2086.92 339.885 Q2089.28 341.112 2091.58 341.737 Q2093.87 342.339 2096.11 342.339 Q2099.12 342.339 2100.74 341.32 Q2102.36 340.278 2102.36 338.403 Q2102.36 336.667 2101.18 335.741 Q2100.03 334.816 2096.07 333.959 L2094.59 333.612 Q2090.63 332.778 2088.87 331.066 Q2087.11 329.329 2087.11 326.32 Q2087.11 322.663 2089.7 320.672 Q2092.29 318.681 2097.06 318.681 Q2099.42 318.681 2101.51 319.029 Q2103.59 319.376 2105.35 320.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2115.42 311.945 L2115.42 319.306 L2124.19 319.306 L2124.19 322.617 L2115.42 322.617 L2115.42 336.691 Q2115.42 339.862 2116.28 340.765 Q2117.15 341.667 2119.82 341.667 L2124.19 341.667 L2124.19 345.232 L2119.82 345.232 Q2114.89 345.232 2113.01 343.403 Q2111.14 341.552 2111.14 336.691 L2111.14 322.617 L2108.01 322.617 L2108.01 319.306 L2111.14 319.306 L2111.14 311.945 L2115.42 311.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2143.73 309.214 L2147.99 309.214 L2147.99 345.232 L2143.73 345.232 L2143.73 309.214 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2162.5 322.292 Q2159.08 322.292 2157.09 324.978 Q2155.09 327.64 2155.09 332.292 Q2155.09 336.945 2157.06 339.63 Q2159.05 342.292 2162.5 342.292 Q2165.9 342.292 2167.9 339.607 Q2169.89 336.922 2169.89 332.292 Q2169.89 327.686 2167.9 325.001 Q2165.9 322.292 2162.5 322.292 M2162.5 318.681 Q2168.06 318.681 2171.23 322.292 Q2174.4 325.904 2174.4 332.292 Q2174.4 338.658 2171.23 342.292 Q2168.06 345.903 2162.5 345.903 Q2156.92 345.903 2153.75 342.292 Q2150.6 338.658 2150.6 332.292 Q2150.6 325.904 2153.75 322.292 Q2156.92 318.681 2162.5 318.681 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2195.39 320.07 L2195.39 324.098 Q2193.59 323.172 2191.65 322.709 Q2189.7 322.246 2187.62 322.246 Q2184.45 322.246 2182.85 323.218 Q2181.27 324.191 2181.27 326.135 Q2181.27 327.616 2182.41 328.473 Q2183.54 329.306 2186.97 330.07 L2188.43 330.394 Q2192.96 331.366 2194.86 333.149 Q2196.78 334.908 2196.78 338.079 Q2196.78 341.69 2193.91 343.797 Q2191.07 345.903 2186.07 345.903 Q2183.98 345.903 2181.71 345.487 Q2179.47 345.093 2176.97 344.283 L2176.97 339.885 Q2179.33 341.112 2181.62 341.737 Q2183.91 342.339 2186.16 342.339 Q2189.17 342.339 2190.79 341.32 Q2192.41 340.278 2192.41 338.403 Q2192.41 336.667 2191.23 335.741 Q2190.07 334.816 2186.11 333.959 L2184.63 333.612 Q2180.67 332.778 2178.91 331.066 Q2177.15 329.329 2177.15 326.32 Q2177.15 322.663 2179.75 320.672 Q2182.34 318.681 2187.11 318.681 Q2189.47 318.681 2191.55 319.029 Q2193.64 319.376 2195.39 320.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2200)\" d=\"M 0 0 M2217.78 320.07 L2217.78 324.098 Q2215.97 323.172 2214.03 322.709 Q2212.08 322.246 2210 322.246 Q2206.83 322.246 2205.23 323.218 Q2203.66 324.191 2203.66 326.135 Q2203.66 327.616 2204.79 328.473 Q2205.93 329.306 2209.35 330.07 L2210.81 330.394 Q2215.35 331.366 2217.25 333.149 Q2219.17 334.908 2219.17 338.079 Q2219.17 341.69 2216.3 343.797 Q2213.45 345.903 2208.45 345.903 Q2206.37 345.903 2204.1 345.487 Q2201.85 345.093 2199.35 344.283 L2199.35 339.885 Q2201.71 341.112 2204.01 341.737 Q2206.3 342.339 2208.54 342.339 Q2211.55 342.339 2213.17 341.32 Q2214.79 340.278 2214.79 338.403 Q2214.79 336.667 2213.61 335.741 Q2212.46 334.816 2208.5 333.959 L2207.02 333.612 Q2203.06 332.778 2201.3 331.066 Q2199.54 329.329 2199.54 326.32 Q2199.54 322.663 2202.13 320.672 Q2204.72 318.681 2209.49 318.681 Q2211.85 318.681 2213.94 319.029 Q2216.02 319.376 2217.78 320.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(1:length(train_loss_results), train_loss_results, title=\"Loss\", label=\"Train loss\")\n",
    "plot!(1:length(test_loss_results), test_loss_results, label=\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
