{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: onehotbatch, onecold, @epochs\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using MLDatasets\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and prepare the data\n",
    "\n",
    "We load the MNIST train and test data using MLDatasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load full train set\n",
    "train_x, train_y = MLDatasets.MNIST.traindata(Float32)\n",
    "\n",
    "# load full test set\n",
    "test_x, test_y = MLDatasets.MNIST.testdata(Float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: (28, 28, 60000)\n",
      "Size of test set: (28, 28, 10000)\n"
     ]
    }
   ],
   "source": [
    "println(\"Size of train set: $(size(train_x))\")\n",
    "println(\"Size of test set: $(size(test_x))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the train dataset has 60000 examples and the test dataset has 10000. Each element of both datasets is a 28x28 matrix. Flux expects the data to be in a different shape (see image below). \n",
    "\n",
    "\n",
    "\n",
    "We need to transform the MNIST data so we can feed it into our Flux model. Thus, we flatten the input data, that is, we convert each 28x28 matrix into a 784-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784×10000 Array{Float32,2}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱            ⋮                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten each image into a vector\n",
    "train_x = Flux.flatten(train_x)\n",
    "test_x = Flux.flatten(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to one-hot encode (see image above - Chris Albon) the labels so that our model can understand them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bool[0 1 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 1; 0 0 … 0 0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode the labels\n",
    "train_y, test_y = onehotbatch(train_y, 0:9), onehotbatch(test_y, 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we are working with a very large dataset, it is more convenient to work with mini batches of the data. We use Flux's [DataLoader](https://fluxml.ai/Flux.jl/stable/data/dataloader/) type so we can iterate over the mini batches of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoader{Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}}((Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0]), 256, 10000, true, 10000, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000], false)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders (mini-batch iterators)\n",
    "train_data_loader = DataLoader((train_x, train_y), batchsize=256, shuffle=true)\n",
    "test_data_loader = DataLoader((test_x, test_y), batchsize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "As we mentioned above, our model has one input layer, one hidden layer (with 32 perceptrons), and an output layer (with n_classes perceptrons). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.0017218534 0.05448177 … 0.057853024 -0.03525453; -0.06551546 -0.07365144 … 0.05125279 -0.018042415; … ; -0.049772117 -0.012488579 … 0.0053936155 -0.050251473; 0.02933832 0.011481742 … 0.024355408 0.0021731392], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.012481836 -0.0713582 … 0.1783298 0.3768975; 0.14204675 -0.3283766 … 0.28812033 -0.23370884; … ; 0.04115477 0.19755809 … -0.17236012 0.17841288; -0.37440264 0.23192486 … 0.010134373 -0.06286894], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct model\n",
    "img_size = (28,28,1)\n",
    "n_classes = 10\n",
    "\n",
    "model = Chain( Dense(prod(img_size), 32, relu),\n",
    "                  Dense(32, n_classes))\n",
    "\n",
    "ps = Flux.params(model) # model's trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a loss function and an accuracy function. It is very important to monitor loss during training time so we can decide when it's best to stop the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(data_loader, model)\n",
    "    total_loss = 0.0f0\n",
    "    #change num to something more meaningful\n",
    "    num_elements = 0\n",
    "    for (x, y) in data_loader\n",
    "        ŷ = model(x)\n",
    "        total_loss += logitcrossentropy(ŷ, y, agg=sum)\n",
    "        num_elements +=  size(x)[end]\n",
    "    end\n",
    "    return total_loss / num_elements\n",
    "end\n",
    "\n",
    "\n",
    "function accuracy(data_loader, model)\n",
    "    accuracy = 0\n",
    "    num_elements = 0\n",
    "    for (x, y) in data_loader\n",
    "        ŷ = model(x)\n",
    "        accuracy += sum(onecold(ŷ) .== onecold(y))\n",
    "        num_elements += size(x)[end]\n",
    "    end   \n",
    "    \n",
    "    return accuracy / num_elements\n",
    "end\n",
    "\n",
    "# Comment on why we need both: loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we use the gradient descent algorithm. It does the following: \n",
    "\n",
    "For this example, we use the ADAM optimiser and set a value η for the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0003, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Optimizer\n",
    "η = 3e-4 \n",
    "\n",
    "opt = ADAM(η)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model. We train a neural network in stages (or epochs). In each epoch, we perform one step of the gradient descent algorithm and output the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1\n",
      "  train_loss = 0.5755098, train_accuracy = 0.8666\n",
      "  test_loss = 0.5563042, test_accuracy = 0.8717\n",
      "Epoch=2\n",
      "  train_loss = 0.38811642, train_accuracy = 0.8984\n",
      "  test_loss = 0.37503633, test_accuracy = 0.9018\n",
      "Epoch=3\n",
      "  train_loss = 0.32634193, train_accuracy = 0.9111666666666667\n",
      "  test_loss = 0.31587765, test_accuracy = 0.9145\n",
      "Epoch=4\n",
      "  train_loss = 0.29346618, train_accuracy = 0.9189833333333334\n",
      "  test_loss = 0.2865617, test_accuracy = 0.9224\n",
      "Epoch=5\n",
      "  train_loss = 0.27072933, train_accuracy = 0.9254833333333333\n",
      "  test_loss = 0.26759276, test_accuracy = 0.9254\n",
      "Epoch=6\n",
      "  train_loss = 0.2533457, train_accuracy = 0.9304333333333333\n",
      "  test_loss = 0.25219682, test_accuracy = 0.931\n",
      "Epoch=7\n",
      "  train_loss = 0.23954394, train_accuracy = 0.9332666666666667\n",
      "  test_loss = 0.2388809, test_accuracy = 0.9338\n",
      "Epoch=8\n",
      "  train_loss = 0.2272161, train_accuracy = 0.9364666666666667\n",
      "  test_loss = 0.22827408, test_accuracy = 0.9353\n",
      "Epoch=9\n",
      "  train_loss = 0.21546029, train_accuracy = 0.9405833333333333\n",
      "  test_loss = 0.21842596, test_accuracy = 0.9378\n",
      "Epoch=10\n",
      "  train_loss = 0.2058932, train_accuracy = 0.9426166666666667\n",
      "  test_loss = 0.21017373, test_accuracy = 0.9409\n",
      "Epoch=11\n",
      "  train_loss = 0.19732837, train_accuracy = 0.9449\n",
      "  test_loss = 0.20284666, test_accuracy = 0.942\n",
      "Epoch=12\n",
      "  train_loss = 0.18975253, train_accuracy = 0.9468166666666666\n",
      "  test_loss = 0.1953355, test_accuracy = 0.9447\n",
      "Epoch=13\n",
      "  train_loss = 0.18285397, train_accuracy = 0.9484833333333333\n",
      "  test_loss = 0.19031072, test_accuracy = 0.9461\n",
      "Epoch=14\n",
      "  train_loss = 0.1760951, train_accuracy = 0.9508333333333333\n",
      "  test_loss = 0.18400095, test_accuracy = 0.9478\n",
      "Epoch=15\n",
      "  train_loss = 0.17021842, train_accuracy = 0.9525666666666667\n",
      "  test_loss = 0.18024473, test_accuracy = 0.9499\n",
      "Epoch=16\n",
      "  train_loss = 0.16465221, train_accuracy = 0.9544333333333334\n",
      "  test_loss = 0.17561066, test_accuracy = 0.9497\n",
      "Epoch=17\n",
      "  train_loss = 0.15987118, train_accuracy = 0.9557833333333333\n",
      "  test_loss = 0.17054099, test_accuracy = 0.9517\n",
      "Epoch=18\n",
      "  train_loss = 0.15553154, train_accuracy = 0.9570833333333333\n",
      "  test_loss = 0.16710983, test_accuracy = 0.9518\n",
      "Epoch=19\n",
      "  train_loss = 0.15051307, train_accuracy = 0.9583333333333334\n",
      "  test_loss = 0.16102797, test_accuracy = 0.9545\n",
      "Epoch=20\n",
      "  train_loss = 0.14660808, train_accuracy = 0.9590166666666666\n",
      "  test_loss = 0.15997276, test_accuracy = 0.9544\n",
      "Epoch=21\n",
      "  train_loss = 0.14245252, train_accuracy = 0.9600166666666666\n",
      "  test_loss = 0.15655336, test_accuracy = 0.9554\n",
      "Epoch=22\n",
      "  train_loss = 0.13872, train_accuracy = 0.9613666666666667\n",
      "  test_loss = 0.15256365, test_accuracy = 0.9561\n",
      "Epoch=23\n",
      "  train_loss = 0.13621956, train_accuracy = 0.9617166666666667\n",
      "  test_loss = 0.15105367, test_accuracy = 0.9571\n",
      "Epoch=24\n",
      "  train_loss = 0.13261463, train_accuracy = 0.9626\n",
      "  test_loss = 0.14827971, test_accuracy = 0.9564\n",
      "Epoch=25\n",
      "  train_loss = 0.12982817, train_accuracy = 0.9644166666666667\n",
      "  test_loss = 0.1454095, test_accuracy = 0.9575\n",
      "Epoch=26\n",
      "  train_loss = 0.12599805, train_accuracy = 0.9643833333333334\n",
      "  test_loss = 0.14234376, test_accuracy = 0.9576\n",
      "Epoch=27\n",
      "  train_loss = 0.12327431, train_accuracy = 0.96545\n",
      "  test_loss = 0.14037868, test_accuracy = 0.9584\n",
      "Epoch=28\n",
      "  train_loss = 0.121358536, train_accuracy = 0.9654666666666667\n",
      "  test_loss = 0.13898693, test_accuracy = 0.9585\n",
      "Epoch=29\n",
      "  train_loss = 0.117939934, train_accuracy = 0.9668833333333333\n",
      "  test_loss = 0.13705467, test_accuracy = 0.9596\n",
      "Epoch=30\n",
      "  train_loss = 0.115803204, train_accuracy = 0.9673333333333334\n",
      "  test_loss = 0.13497928, test_accuracy = 0.9595\n",
      "Epoch=31\n",
      "  train_loss = 0.11309064, train_accuracy = 0.9683833333333334\n",
      "  test_loss = 0.13302955, test_accuracy = 0.9605\n",
      "Epoch=32\n",
      "  train_loss = 0.1107822, train_accuracy = 0.9687\n",
      "  test_loss = 0.13056146, test_accuracy = 0.9609\n",
      "Epoch=33\n",
      "  train_loss = 0.108508006, train_accuracy = 0.9696333333333333\n",
      "  test_loss = 0.13019878, test_accuracy = 0.9609\n",
      "Epoch=34\n",
      "  train_loss = 0.10782431, train_accuracy = 0.9698166666666667\n",
      "  test_loss = 0.12837724, test_accuracy = 0.9614\n",
      "Epoch=35\n",
      "  train_loss = 0.10435624, train_accuracy = 0.9707833333333333\n",
      "  test_loss = 0.12755959, test_accuracy = 0.9614\n",
      "Epoch=36\n",
      "  train_loss = 0.10310008, train_accuracy = 0.9711333333333333\n",
      "  test_loss = 0.12581244, test_accuracy = 0.9625\n",
      "Epoch=37\n",
      "  train_loss = 0.10047655, train_accuracy = 0.9721\n",
      "  test_loss = 0.123929456, test_accuracy = 0.9628\n",
      "Epoch=38\n",
      "  train_loss = 0.09878269, train_accuracy = 0.9721333333333333\n",
      "  test_loss = 0.12420811, test_accuracy = 0.9624\n",
      "Epoch=39\n",
      "  train_loss = 0.097277924, train_accuracy = 0.9729166666666667\n",
      "  test_loss = 0.123796545, test_accuracy = 0.9631\n",
      "Epoch=40\n",
      "  train_loss = 0.09551358, train_accuracy = 0.9732333333333333\n",
      "  test_loss = 0.12256487, test_accuracy = 0.9629\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "train_accuracy_results = []\n",
    "test_accuracy_results = []\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    for (x, y) in train_data_loader\n",
    "        gs = gradient(() -> logitcrossentropy(model(x), y), ps) # compute gradient\n",
    "        Flux.Optimise.update!(opt, ps, gs) # update parameters\n",
    "     end\n",
    "        \n",
    "    # Compute accuracy and loss for all of the train and test data\n",
    "    train_loss = loss(train_data_loader, model)\n",
    "    train_acc = accuracy(train_data_loader, model)\n",
    "    test_loss = loss(test_data_loader, model)\n",
    "    test_acc = accuracy(test_data_loader, model)\n",
    "    println(\"Epoch=$epoch\")\n",
    "    println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "    println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "    push!(train_loss_results, train_loss)\n",
    "    push!(test_loss_results, test_loss)\n",
    "    push!(train_accuracy_results, train_acc)\n",
    "    push!(test_accuracy_results, test_acc)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train and test loss to check overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip4700\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip4700)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip4701\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip4700)\" d=\"\n",
       "M148.334 1486.45 L2352.76 1486.45 L2352.76 123.472 L148.334 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip4702\">\n",
       "    <rect x=\"148\" y=\"123\" width=\"2205\" height=\"1364\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  157.4,1486.45 157.4,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  690.641,1486.45 690.641,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1223.88,1486.45 1223.88,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1757.12,1486.45 1757.12,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.37,1486.45 2290.37,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,1435.85 2352.76,1435.85 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,1167.97 2352.76,1167.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,900.09 2352.76,900.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,632.207 2352.76,632.207 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,364.324 2352.76,364.324 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1486.45 148.334,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  157.4,1486.45 157.4,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  690.641,1486.45 690.641,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1223.88,1486.45 1223.88,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1757.12,1486.45 1757.12,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.37,1486.45 2290.37,1470.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1435.85 174.787,1435.85 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1167.97 174.787,1167.97 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,900.09 174.787,900.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,632.207 174.787,632.207 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,364.324 174.787,364.324 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip4700)\" d=\"M 0 0 M157.4 1508.44 Q153.788 1508.44 151.96 1512 Q150.154 1515.55 150.154 1522.67 Q150.154 1529.78 151.96 1533.35 Q153.788 1536.89 157.4 1536.89 Q161.034 1536.89 162.839 1533.35 Q164.668 1529.78 164.668 1522.67 Q164.668 1515.55 162.839 1512 Q161.034 1508.44 157.4 1508.44 M157.4 1504.73 Q163.21 1504.73 166.265 1509.34 Q169.344 1513.92 169.344 1522.67 Q169.344 1531.4 166.265 1536.01 Q163.21 1540.59 157.4 1540.59 Q151.589 1540.59 148.511 1536.01 Q145.455 1531.4 145.455 1522.67 Q145.455 1513.92 148.511 1509.34 Q151.589 1504.73 157.4 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M667.516 1535.98 L675.155 1535.98 L675.155 1509.62 L666.845 1511.29 L666.845 1507.03 L675.109 1505.36 L679.785 1505.36 L679.785 1535.98 L687.424 1535.98 L687.424 1539.92 L667.516 1539.92 L667.516 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M702.493 1508.44 Q698.882 1508.44 697.053 1512 Q695.248 1515.55 695.248 1522.67 Q695.248 1529.78 697.053 1533.35 Q698.882 1536.89 702.493 1536.89 Q706.127 1536.89 707.933 1533.35 Q709.762 1529.78 709.762 1522.67 Q709.762 1515.55 707.933 1512 Q706.127 1508.44 702.493 1508.44 M702.493 1504.73 Q708.303 1504.73 711.359 1509.34 Q714.437 1513.92 714.437 1522.67 Q714.437 1531.4 711.359 1536.01 Q708.303 1540.59 702.493 1540.59 Q696.683 1540.59 693.604 1536.01 Q690.549 1531.4 690.549 1522.67 Q690.549 1513.92 693.604 1509.34 Q696.683 1504.73 702.493 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1205.03 1535.98 L1221.35 1535.98 L1221.35 1539.92 L1199.4 1539.92 L1199.4 1535.98 Q1202.07 1533.23 1206.65 1528.6 Q1211.26 1523.95 1212.44 1522.61 Q1214.68 1520.08 1215.56 1518.35 Q1216.46 1516.59 1216.46 1514.9 Q1216.46 1512.14 1214.52 1510.41 Q1212.6 1508.67 1209.5 1508.67 Q1207.3 1508.67 1204.84 1509.43 Q1202.41 1510.2 1199.64 1511.75 L1199.64 1507.03 Q1202.46 1505.89 1204.91 1505.31 Q1207.37 1504.73 1209.4 1504.73 Q1214.77 1504.73 1217.97 1507.42 Q1221.16 1510.11 1221.16 1514.6 Q1221.16 1516.73 1220.35 1518.65 Q1219.57 1520.54 1217.46 1523.14 Q1216.88 1523.81 1213.78 1527.03 Q1210.68 1530.22 1205.03 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1236.42 1508.44 Q1232.81 1508.44 1230.98 1512 Q1229.17 1515.55 1229.17 1522.67 Q1229.17 1529.78 1230.98 1533.35 Q1232.81 1536.89 1236.42 1536.89 Q1240.05 1536.89 1241.86 1533.35 Q1243.69 1529.78 1243.69 1522.67 Q1243.69 1515.55 1241.86 1512 Q1240.05 1508.44 1236.42 1508.44 M1236.42 1504.73 Q1242.23 1504.73 1245.28 1509.34 Q1248.36 1513.92 1248.36 1522.67 Q1248.36 1531.4 1245.28 1536.01 Q1242.23 1540.59 1236.42 1540.59 Q1230.61 1540.59 1227.53 1536.01 Q1224.47 1531.4 1224.47 1522.67 Q1224.47 1513.92 1227.53 1509.34 Q1230.61 1504.73 1236.42 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1747.87 1521.29 Q1751.22 1522 1753.1 1524.27 Q1755 1526.54 1755 1529.87 Q1755 1534.99 1751.48 1537.79 Q1747.96 1540.59 1741.48 1540.59 Q1739.3 1540.59 1736.99 1540.15 Q1734.69 1539.73 1732.24 1538.88 L1732.24 1534.36 Q1734.19 1535.5 1736.5 1536.08 Q1738.81 1536.66 1741.34 1536.66 Q1745.74 1536.66 1748.03 1534.92 Q1750.34 1533.18 1750.34 1529.87 Q1750.34 1526.82 1748.19 1525.11 Q1746.06 1523.37 1742.24 1523.37 L1738.21 1523.37 L1738.21 1519.53 L1742.43 1519.53 Q1745.87 1519.53 1747.7 1518.16 Q1749.53 1516.77 1749.53 1514.18 Q1749.53 1511.52 1747.63 1510.11 Q1745.76 1508.67 1742.24 1508.67 Q1740.32 1508.67 1738.12 1509.09 Q1735.92 1509.5 1733.28 1510.38 L1733.28 1506.22 Q1735.94 1505.48 1738.26 1505.11 Q1740.6 1504.73 1742.66 1504.73 Q1747.98 1504.73 1751.08 1507.17 Q1754.19 1509.57 1754.19 1513.69 Q1754.19 1516.56 1752.54 1518.55 Q1750.9 1520.52 1747.87 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1770.06 1508.44 Q1766.45 1508.44 1764.62 1512 Q1762.82 1515.55 1762.82 1522.67 Q1762.82 1529.78 1764.62 1533.35 Q1766.45 1536.89 1770.06 1536.89 Q1773.7 1536.89 1775.5 1533.35 Q1777.33 1529.78 1777.33 1522.67 Q1777.33 1515.55 1775.5 1512 Q1773.7 1508.44 1770.06 1508.44 M1770.06 1504.73 Q1775.87 1504.73 1778.93 1509.34 Q1782.01 1513.92 1782.01 1522.67 Q1782.01 1531.4 1778.93 1536.01 Q1775.87 1540.59 1770.06 1540.59 Q1764.25 1540.59 1761.18 1536.01 Q1758.12 1531.4 1758.12 1522.67 Q1758.12 1513.92 1761.18 1509.34 Q1764.25 1504.73 1770.06 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2279.87 1509.43 L2268.06 1527.88 L2279.87 1527.88 L2279.87 1509.43 M2278.64 1505.36 L2284.52 1505.36 L2284.52 1527.88 L2289.45 1527.88 L2289.45 1531.77 L2284.52 1531.77 L2284.52 1539.92 L2279.87 1539.92 L2279.87 1531.77 L2264.27 1531.77 L2264.27 1527.26 L2278.64 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2304.52 1508.44 Q2300.91 1508.44 2299.08 1512 Q2297.28 1515.55 2297.28 1522.67 Q2297.28 1529.78 2299.08 1533.35 Q2300.91 1536.89 2304.52 1536.89 Q2308.16 1536.89 2309.96 1533.35 Q2311.79 1529.78 2311.79 1522.67 Q2311.79 1515.55 2309.96 1512 Q2308.16 1508.44 2304.52 1508.44 M2304.52 1504.73 Q2310.33 1504.73 2313.39 1509.34 Q2316.47 1513.92 2316.47 1522.67 Q2316.47 1531.4 2313.39 1536.01 Q2310.33 1540.59 2304.52 1540.59 Q2298.71 1540.59 2295.63 1536.01 Q2292.58 1531.4 2292.58 1522.67 Q2292.58 1513.92 2295.63 1509.34 Q2298.71 1504.73 2304.52 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M76.6495 1421.65 Q73.0384 1421.65 71.2097 1425.22 Q69.4041 1428.76 69.4041 1435.89 Q69.4041 1443 71.2097 1446.56 Q73.0384 1450.1 76.6495 1450.1 Q80.2837 1450.1 82.0892 1446.56 Q83.9179 1443 83.9179 1435.89 Q83.9179 1428.76 82.0892 1425.22 Q80.2837 1421.65 76.6495 1421.65 M76.6495 1417.95 Q82.4596 1417.95 85.5152 1422.56 Q88.5938 1427.14 88.5938 1435.89 Q88.5938 1444.62 85.5152 1449.22 Q82.4596 1453.81 76.6495 1453.81 Q70.8393 1453.81 67.7606 1449.22 Q64.7051 1444.62 64.7051 1435.89 Q64.7051 1427.14 67.7606 1422.56 Q70.8393 1417.95 76.6495 1417.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M93.6633 1447.26 L98.5475 1447.26 L98.5475 1453.13 L93.6633 1453.13 L93.6633 1447.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M104.427 1449.2 L112.066 1449.2 L112.066 1422.83 L103.756 1424.5 L103.756 1420.24 L112.02 1418.57 L116.696 1418.57 L116.696 1449.2 L124.334 1449.2 L124.334 1453.13 L104.427 1453.13 L104.427 1449.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M77.0198 1153.77 Q73.4087 1153.77 71.58 1157.34 Q69.7745 1160.88 69.7745 1168.01 Q69.7745 1175.11 71.58 1178.68 Q73.4087 1182.22 77.0198 1182.22 Q80.6541 1182.22 82.4596 1178.68 Q84.2883 1175.11 84.2883 1168.01 Q84.2883 1160.88 82.4596 1157.34 Q80.6541 1153.77 77.0198 1153.77 M77.0198 1150.07 Q82.83 1150.07 85.8855 1154.67 Q88.9642 1159.26 88.9642 1168.01 Q88.9642 1176.73 85.8855 1181.34 Q82.83 1185.92 77.0198 1185.92 Q71.2097 1185.92 68.131 1181.34 Q65.0754 1176.73 65.0754 1168.01 Q65.0754 1159.26 68.131 1154.67 Q71.2097 1150.07 77.0198 1150.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M94.0336 1179.37 L98.9179 1179.37 L98.9179 1185.25 L94.0336 1185.25 L94.0336 1179.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M108.015 1181.32 L124.334 1181.32 L124.334 1185.25 L102.39 1185.25 L102.39 1181.32 Q105.052 1178.56 109.635 1173.93 Q114.242 1169.28 115.422 1167.94 Q117.668 1165.41 118.547 1163.68 Q119.45 1161.92 119.45 1160.23 Q119.45 1157.47 117.506 1155.74 Q115.584 1154 112.483 1154 Q110.284 1154 107.83 1154.77 Q105.399 1155.53 102.622 1157.08 L102.622 1152.36 Q105.446 1151.22 107.899 1150.65 Q110.353 1150.07 112.39 1150.07 Q117.76 1150.07 120.955 1152.75 Q124.149 1155.44 124.149 1159.93 Q124.149 1162.06 123.339 1163.98 Q122.552 1165.88 120.446 1168.47 Q119.867 1169.14 116.765 1172.36 Q113.663 1175.55 108.015 1181.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M76.0708 885.888 Q72.4597 885.888 70.631 889.453 Q68.8254 892.995 68.8254 900.124 Q68.8254 907.231 70.631 910.796 Q72.4597 914.337 76.0708 914.337 Q79.705 914.337 81.5105 910.796 Q83.3392 907.231 83.3392 900.124 Q83.3392 892.995 81.5105 889.453 Q79.705 885.888 76.0708 885.888 M76.0708 882.185 Q81.8809 882.185 84.9365 886.791 Q88.0151 891.374 88.0151 900.124 Q88.0151 908.851 84.9365 913.458 Q81.8809 918.041 76.0708 918.041 Q70.2606 918.041 67.1819 913.458 Q64.1264 908.851 64.1264 900.124 Q64.1264 891.374 67.1819 886.791 Q70.2606 882.185 76.0708 882.185 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M93.0846 911.49 L97.9688 911.49 L97.9688 917.37 L93.0846 917.37 L93.0846 911.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M117.205 898.735 Q120.561 899.453 122.436 901.722 Q124.334 903.99 124.334 907.323 Q124.334 912.439 120.816 915.24 Q117.297 918.041 110.816 918.041 Q108.64 918.041 106.325 917.601 Q104.034 917.184 101.58 916.328 L101.58 911.814 Q103.524 912.948 105.839 913.527 Q108.154 914.106 110.677 914.106 Q115.075 914.106 117.367 912.37 Q119.682 910.634 119.682 907.323 Q119.682 904.268 117.529 902.555 Q115.399 900.819 111.58 900.819 L107.552 900.819 L107.552 896.976 L111.765 896.976 Q115.214 896.976 117.043 895.61 Q118.871 894.222 118.871 891.629 Q118.871 888.967 116.973 887.555 Q115.098 886.12 111.58 886.12 Q109.659 886.12 107.459 886.536 Q105.26 886.953 102.622 887.833 L102.622 883.666 Q105.284 882.925 107.598 882.555 Q109.936 882.185 111.996 882.185 Q117.321 882.185 120.422 884.615 Q123.524 887.023 123.524 891.143 Q123.524 894.013 121.881 896.004 Q120.237 897.972 117.205 898.735 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M74.9365 618.006 Q71.3254 618.006 69.4967 621.571 Q67.6912 625.112 67.6912 632.242 Q67.6912 639.348 69.4967 642.913 Q71.3254 646.455 74.9365 646.455 Q78.5707 646.455 80.3763 642.913 Q82.205 639.348 82.205 632.242 Q82.205 625.112 80.3763 621.571 Q78.5707 618.006 74.9365 618.006 M74.9365 614.302 Q80.7467 614.302 83.8022 618.909 Q86.8809 623.492 86.8809 632.242 Q86.8809 640.969 83.8022 645.575 Q80.7467 650.158 74.9365 650.158 Q69.1264 650.158 66.0477 645.575 Q62.9921 640.969 62.9921 632.242 Q62.9921 623.492 66.0477 618.909 Q69.1264 614.302 74.9365 614.302 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M91.9503 643.607 L96.8345 643.607 L96.8345 649.487 L91.9503 649.487 L91.9503 643.607 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M114.751 619.001 L102.946 637.45 L114.751 637.45 L114.751 619.001 M113.524 614.927 L119.404 614.927 L119.404 637.45 L124.334 637.45 L124.334 641.339 L119.404 641.339 L119.404 649.487 L114.751 649.487 L114.751 641.339 L99.1493 641.339 L99.1493 636.825 L113.524 614.927 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M76.418 350.123 Q72.8069 350.123 70.9782 353.688 Q69.1726 357.23 69.1726 364.359 Q69.1726 371.466 70.9782 375.03 Q72.8069 378.572 76.418 378.572 Q80.0522 378.572 81.8578 375.03 Q83.6865 371.466 83.6865 364.359 Q83.6865 357.23 81.8578 353.688 Q80.0522 350.123 76.418 350.123 M76.418 346.419 Q82.2281 346.419 85.2837 351.026 Q88.3624 355.609 88.3624 364.359 Q88.3624 373.086 85.2837 377.692 Q82.2281 382.276 76.418 382.276 Q70.6078 382.276 67.5291 377.692 Q64.4736 373.086 64.4736 364.359 Q64.4736 355.609 67.5291 351.026 Q70.6078 346.419 76.418 346.419 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M93.4318 375.725 L98.316 375.725 L98.316 381.604 L93.4318 381.604 L93.4318 375.725 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M103.432 347.044 L121.788 347.044 L121.788 350.98 L107.714 350.98 L107.714 359.452 Q108.733 359.105 109.751 358.943 Q110.77 358.757 111.788 358.757 Q117.575 358.757 120.955 361.929 Q124.334 365.1 124.334 370.517 Q124.334 376.095 120.862 379.197 Q117.39 382.276 111.071 382.276 Q108.895 382.276 106.626 381.905 Q104.381 381.535 101.973 380.794 L101.973 376.095 Q104.057 377.229 106.279 377.785 Q108.501 378.341 110.978 378.341 Q114.983 378.341 117.321 376.234 Q119.658 374.128 119.658 370.517 Q119.658 366.905 117.321 364.799 Q114.983 362.693 110.978 362.693 Q109.103 362.693 107.228 363.109 Q105.376 363.526 103.432 364.405 L103.432 347.044 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1170.18 12.096 L1178.36 12.096 L1178.36 65.6895 L1207.81 65.6895 L1207.81 72.576 L1170.18 72.576 L1170.18 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1231.75 32.4315 Q1225.75 32.4315 1222.27 37.1306 Q1218.79 41.7891 1218.79 49.9314 Q1218.79 58.0738 1222.23 62.7728 Q1225.71 67.4314 1231.75 67.4314 Q1237.7 67.4314 1241.19 62.7323 Q1244.67 58.0333 1244.67 49.9314 Q1244.67 41.8701 1241.19 37.1711 Q1237.7 32.4315 1231.75 32.4315 M1231.75 26.1121 Q1241.47 26.1121 1247.02 32.4315 Q1252.57 38.7509 1252.57 49.9314 Q1252.57 61.0714 1247.02 67.4314 Q1241.47 73.7508 1231.75 73.7508 Q1221.99 73.7508 1216.44 67.4314 Q1210.93 61.0714 1210.93 49.9314 Q1210.93 38.7509 1216.44 32.4315 Q1221.99 26.1121 1231.75 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1289.31 28.5427 L1289.31 35.5912 Q1286.15 33.9709 1282.75 33.1607 Q1279.35 32.3505 1275.7 32.3505 Q1270.15 32.3505 1267.36 34.0519 Q1264.6 35.7533 1264.6 39.156 Q1264.6 41.7486 1266.59 43.2475 Q1268.57 44.7058 1274.57 46.0426 L1277.12 46.6097 Q1285.06 48.3111 1288.38 51.4303 Q1291.74 54.509 1291.74 60.0587 Q1291.74 66.3781 1286.72 70.0644 Q1281.74 73.7508 1272.99 73.7508 Q1269.34 73.7508 1265.37 73.0216 Q1261.44 72.3329 1257.07 70.9151 L1257.07 63.2184 Q1261.2 65.3654 1265.21 66.4591 Q1269.22 67.5124 1273.15 67.5124 Q1278.42 67.5124 1281.25 65.73 Q1284.09 63.9071 1284.09 60.6258 Q1284.09 57.5877 1282.02 55.9673 Q1280 54.3469 1273.07 52.8481 L1270.48 52.2405 Q1263.55 50.7821 1260.47 47.7845 Q1257.39 44.7463 1257.39 39.4801 Q1257.39 33.0797 1261.93 29.5959 Q1266.47 26.1121 1274.81 26.1121 Q1278.94 26.1121 1282.59 26.7198 Q1286.23 27.3274 1289.31 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M1328.48 28.5427 L1328.48 35.5912 Q1325.32 33.9709 1321.92 33.1607 Q1318.52 32.3505 1314.87 32.3505 Q1309.32 32.3505 1306.53 34.0519 Q1303.77 35.7533 1303.77 39.156 Q1303.77 41.7486 1305.76 43.2475 Q1307.74 44.7058 1313.74 46.0426 L1316.29 46.6097 Q1324.23 48.3111 1327.55 51.4303 Q1330.92 54.509 1330.92 60.0587 Q1330.92 66.3781 1325.89 70.0644 Q1320.91 73.7508 1312.16 73.7508 Q1308.51 73.7508 1304.54 73.0216 Q1300.61 72.3329 1296.24 70.9151 L1296.24 63.2184 Q1300.37 65.3654 1304.38 66.4591 Q1308.39 67.5124 1312.32 67.5124 Q1317.59 67.5124 1320.42 65.73 Q1323.26 63.9071 1323.26 60.6258 Q1323.26 57.5877 1321.19 55.9673 Q1319.17 54.3469 1312.24 52.8481 L1309.65 52.2405 Q1302.72 50.7821 1299.64 47.7845 Q1296.56 44.7463 1296.56 39.4801 Q1296.56 33.0797 1301.1 29.5959 Q1305.64 26.1121 1313.98 26.1121 Q1318.11 26.1121 1321.76 26.7198 Q1325.41 27.3274 1328.48 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip4702)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.724,162.047 264.048,664.041 317.372,829.524 370.696,917.593 424.02,978.501 477.345,1025.07 530.669,1062.04 583.993,1095.07 637.317,1126.56 690.641,1152.19 \n",
       "  743.965,1175.13 797.29,1195.42 850.614,1213.9 903.938,1232.01 957.262,1247.75 1010.59,1262.66 1063.91,1275.47 1117.23,1287.1 1170.56,1300.54 1223.88,1311 \n",
       "  1277.21,1322.13 1330.53,1332.13 1383.86,1338.83 1437.18,1348.49 1490.5,1355.95 1543.83,1366.21 1597.15,1373.51 1650.48,1378.64 1703.8,1387.8 1757.12,1393.52 \n",
       "  1810.45,1400.79 1863.77,1406.97 1917.1,1413.06 1970.42,1414.89 2023.75,1424.19 2077.07,1427.55 2130.39,1434.58 2183.72,1439.12 2237.04,1443.15 2290.37,1447.87 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4702)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.724,213.495 264.048,699.08 317.372,857.556 370.696,936.088 424.02,986.903 477.345,1028.15 530.669,1063.82 583.993,1092.23 637.317,1118.61 690.641,1140.72 \n",
       "  743.965,1160.35 797.29,1180.47 850.614,1193.93 903.938,1210.83 957.262,1220.89 1010.59,1233.31 1063.91,1246.89 1117.23,1256.08 1170.56,1272.37 1223.88,1275.2 \n",
       "  1277.21,1284.36 1330.53,1295.05 1383.86,1299.09 1437.18,1306.52 1490.5,1314.21 1543.83,1322.42 1597.15,1327.69 1650.48,1331.42 1703.8,1336.59 1757.12,1342.15 \n",
       "  1810.45,1347.37 1863.77,1353.99 1917.1,1354.96 1970.42,1359.84 2023.75,1362.03 2077.07,1366.71 2130.39,1371.75 2183.72,1371.01 2237.04,1372.11 2290.37,1375.41 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip4700)\" d=\"\n",
       "M1844.53 388.432 L2280.76 388.432 L2280.76 206.992 L1844.53 206.992  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1844.53,388.432 2280.76,388.432 2280.76,206.992 1844.53,206.992 1844.53,388.432 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip4700)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1868.53,267.472 2012.53,267.472 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2036.53 250.192 L2065.77 250.192 L2065.77 254.127 L2053.5 254.127 L2053.5 284.752 L2048.8 284.752 L2048.8 254.127 L2036.53 254.127 L2036.53 250.192 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2078.29 262.808 Q2077.57 262.391 2076.72 262.206 Q2075.88 261.998 2074.86 261.998 Q2071.25 261.998 2069.31 264.359 Q2067.39 266.697 2067.39 271.095 L2067.39 284.752 L2063.1 284.752 L2063.1 258.826 L2067.39 258.826 L2067.39 262.854 Q2068.73 260.493 2070.88 259.359 Q2073.03 258.201 2076.11 258.201 Q2076.55 258.201 2077.09 258.271 Q2077.62 258.317 2078.27 258.433 L2078.29 262.808 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2094.54 271.72 Q2089.38 271.72 2087.39 272.9 Q2085.4 274.081 2085.4 276.928 Q2085.4 279.197 2086.88 280.539 Q2088.38 281.859 2090.95 281.859 Q2094.49 281.859 2096.62 279.359 Q2098.78 276.836 2098.78 272.669 L2098.78 271.72 L2094.54 271.72 M2103.03 269.961 L2103.03 284.752 L2098.78 284.752 L2098.78 280.817 Q2097.32 283.178 2095.14 284.312 Q2092.96 285.423 2089.82 285.423 Q2085.84 285.423 2083.47 283.201 Q2081.14 280.956 2081.14 277.206 Q2081.14 272.831 2084.05 270.609 Q2086.99 268.386 2092.8 268.386 L2098.78 268.386 L2098.78 267.97 Q2098.78 265.03 2096.83 263.433 Q2094.91 261.812 2091.41 261.812 Q2089.19 261.812 2087.09 262.345 Q2084.98 262.877 2083.03 263.942 L2083.03 260.007 Q2085.37 259.104 2087.57 258.664 Q2089.77 258.201 2091.85 258.201 Q2097.48 258.201 2100.26 261.118 Q2103.03 264.035 2103.03 269.961 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2107.5 258.826 L2111.76 258.826 L2111.76 284.752 L2107.5 284.752 L2107.5 258.826 M2107.5 248.734 L2111.76 248.734 L2111.76 254.127 L2107.5 254.127 L2107.5 248.734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2137.78 269.104 L2137.78 284.752 L2133.52 284.752 L2133.52 269.243 Q2133.52 265.562 2132.09 263.734 Q2130.65 261.905 2127.78 261.905 Q2124.33 261.905 2122.34 264.104 Q2120.35 266.303 2120.35 270.099 L2120.35 284.752 L2116.07 284.752 L2116.07 258.826 L2120.35 258.826 L2120.35 262.854 Q2121.88 260.516 2123.94 259.359 Q2126.02 258.201 2128.73 258.201 Q2133.2 258.201 2135.49 260.979 Q2137.78 263.734 2137.78 269.104 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2157.32 248.734 L2161.58 248.734 L2161.58 284.752 L2157.32 284.752 L2157.32 248.734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2176.09 261.812 Q2172.66 261.812 2170.67 264.498 Q2168.68 267.16 2168.68 271.812 Q2168.68 276.465 2170.65 279.15 Q2172.64 281.812 2176.09 281.812 Q2179.49 281.812 2181.48 279.127 Q2183.47 276.442 2183.47 271.812 Q2183.47 267.206 2181.48 264.521 Q2179.49 261.812 2176.09 261.812 M2176.09 258.201 Q2181.65 258.201 2184.82 261.812 Q2187.99 265.424 2187.99 271.812 Q2187.99 278.178 2184.82 281.812 Q2181.65 285.423 2176.09 285.423 Q2170.51 285.423 2167.34 281.812 Q2164.19 278.178 2164.19 271.812 Q2164.19 265.424 2167.34 261.812 Q2170.51 258.201 2176.09 258.201 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2208.98 259.59 L2208.98 263.618 Q2207.18 262.692 2205.23 262.229 Q2203.29 261.766 2201.21 261.766 Q2198.03 261.766 2196.44 262.738 Q2194.86 263.711 2194.86 265.655 Q2194.86 267.136 2196 267.993 Q2197.13 268.826 2200.56 269.59 L2202.02 269.914 Q2206.55 270.886 2208.45 272.669 Q2210.37 274.428 2210.37 277.599 Q2210.37 281.21 2207.5 283.317 Q2204.65 285.423 2199.65 285.423 Q2197.57 285.423 2195.3 285.007 Q2193.06 284.613 2190.56 283.803 L2190.56 279.405 Q2192.92 280.632 2195.21 281.257 Q2197.5 281.859 2199.75 281.859 Q2202.76 281.859 2204.38 280.84 Q2206 279.798 2206 277.923 Q2206 276.187 2204.82 275.261 Q2203.66 274.336 2199.7 273.479 L2198.22 273.132 Q2194.26 272.298 2192.5 270.586 Q2190.74 268.849 2190.74 265.84 Q2190.74 262.183 2193.33 260.192 Q2195.93 258.201 2200.7 258.201 Q2203.06 258.201 2205.14 258.549 Q2207.22 258.896 2208.98 259.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2231.37 259.59 L2231.37 263.618 Q2229.56 262.692 2227.62 262.229 Q2225.67 261.766 2223.59 261.766 Q2220.42 261.766 2218.82 262.738 Q2217.25 263.711 2217.25 265.655 Q2217.25 267.136 2218.38 267.993 Q2219.52 268.826 2222.94 269.59 L2224.4 269.914 Q2228.94 270.886 2230.83 272.669 Q2232.76 274.428 2232.76 277.599 Q2232.76 281.21 2229.89 283.317 Q2227.04 285.423 2222.04 285.423 Q2219.96 285.423 2217.69 285.007 Q2215.44 284.613 2212.94 283.803 L2212.94 279.405 Q2215.3 280.632 2217.59 281.257 Q2219.89 281.859 2222.13 281.859 Q2225.14 281.859 2226.76 280.84 Q2228.38 279.798 2228.38 277.923 Q2228.38 276.187 2227.2 275.261 Q2226.04 274.336 2222.08 273.479 L2220.6 273.132 Q2216.64 272.298 2214.89 270.586 Q2213.13 268.849 2213.13 265.84 Q2213.13 262.183 2215.72 260.192 Q2218.31 258.201 2223.08 258.201 Q2225.44 258.201 2227.52 258.549 Q2229.61 258.896 2231.37 259.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip4700)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1868.53,327.952 2012.53,327.952 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2036.53 310.672 L2065.77 310.672 L2065.77 314.607 L2053.5 314.607 L2053.5 345.232 L2048.8 345.232 L2048.8 314.607 L2036.53 314.607 L2036.53 310.672 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2084.35 331.204 L2084.35 333.288 L2064.77 333.288 Q2065.05 337.686 2067.41 340.001 Q2069.79 342.292 2074.03 342.292 Q2076.48 342.292 2078.78 341.69 Q2081.09 341.089 2083.36 339.885 L2083.36 343.913 Q2081.07 344.885 2078.66 345.394 Q2076.25 345.903 2073.78 345.903 Q2067.57 345.903 2063.94 342.292 Q2060.33 338.681 2060.33 332.524 Q2060.33 326.158 2063.75 322.431 Q2067.2 318.681 2073.03 318.681 Q2078.27 318.681 2081.3 322.061 Q2084.35 325.417 2084.35 331.204 M2080.09 329.954 Q2080.05 326.459 2078.13 324.376 Q2076.23 322.292 2073.08 322.292 Q2069.52 322.292 2067.36 324.306 Q2065.23 326.32 2064.91 329.978 L2080.09 329.954 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2105.35 320.07 L2105.35 324.098 Q2103.54 323.172 2101.6 322.709 Q2099.65 322.246 2097.57 322.246 Q2094.4 322.246 2092.8 323.218 Q2091.23 324.191 2091.23 326.135 Q2091.23 327.616 2092.36 328.473 Q2093.5 329.306 2096.92 330.07 L2098.38 330.394 Q2102.92 331.366 2104.82 333.149 Q2106.74 334.908 2106.74 338.079 Q2106.74 341.69 2103.87 343.797 Q2101.02 345.903 2096.02 345.903 Q2093.94 345.903 2091.67 345.487 Q2089.42 345.093 2086.92 344.283 L2086.92 339.885 Q2089.28 341.112 2091.58 341.737 Q2093.87 342.339 2096.11 342.339 Q2099.12 342.339 2100.74 341.32 Q2102.36 340.278 2102.36 338.403 Q2102.36 336.667 2101.18 335.741 Q2100.03 334.816 2096.07 333.959 L2094.59 333.612 Q2090.63 332.778 2088.87 331.066 Q2087.11 329.329 2087.11 326.32 Q2087.11 322.663 2089.7 320.672 Q2092.29 318.681 2097.06 318.681 Q2099.42 318.681 2101.51 319.029 Q2103.59 319.376 2105.35 320.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2115.42 311.945 L2115.42 319.306 L2124.19 319.306 L2124.19 322.617 L2115.42 322.617 L2115.42 336.691 Q2115.42 339.862 2116.28 340.765 Q2117.15 341.667 2119.82 341.667 L2124.19 341.667 L2124.19 345.232 L2119.82 345.232 Q2114.89 345.232 2113.01 343.403 Q2111.14 341.552 2111.14 336.691 L2111.14 322.617 L2108.01 322.617 L2108.01 319.306 L2111.14 319.306 L2111.14 311.945 L2115.42 311.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2143.73 309.214 L2147.99 309.214 L2147.99 345.232 L2143.73 345.232 L2143.73 309.214 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2162.5 322.292 Q2159.08 322.292 2157.09 324.978 Q2155.09 327.64 2155.09 332.292 Q2155.09 336.945 2157.06 339.63 Q2159.05 342.292 2162.5 342.292 Q2165.9 342.292 2167.9 339.607 Q2169.89 336.922 2169.89 332.292 Q2169.89 327.686 2167.9 325.001 Q2165.9 322.292 2162.5 322.292 M2162.5 318.681 Q2168.06 318.681 2171.23 322.292 Q2174.4 325.904 2174.4 332.292 Q2174.4 338.658 2171.23 342.292 Q2168.06 345.903 2162.5 345.903 Q2156.92 345.903 2153.75 342.292 Q2150.6 338.658 2150.6 332.292 Q2150.6 325.904 2153.75 322.292 Q2156.92 318.681 2162.5 318.681 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2195.39 320.07 L2195.39 324.098 Q2193.59 323.172 2191.65 322.709 Q2189.7 322.246 2187.62 322.246 Q2184.45 322.246 2182.85 323.218 Q2181.27 324.191 2181.27 326.135 Q2181.27 327.616 2182.41 328.473 Q2183.54 329.306 2186.97 330.07 L2188.43 330.394 Q2192.96 331.366 2194.86 333.149 Q2196.78 334.908 2196.78 338.079 Q2196.78 341.69 2193.91 343.797 Q2191.07 345.903 2186.07 345.903 Q2183.98 345.903 2181.71 345.487 Q2179.47 345.093 2176.97 344.283 L2176.97 339.885 Q2179.33 341.112 2181.62 341.737 Q2183.91 342.339 2186.16 342.339 Q2189.17 342.339 2190.79 341.32 Q2192.41 340.278 2192.41 338.403 Q2192.41 336.667 2191.23 335.741 Q2190.07 334.816 2186.11 333.959 L2184.63 333.612 Q2180.67 332.778 2178.91 331.066 Q2177.15 329.329 2177.15 326.32 Q2177.15 322.663 2179.75 320.672 Q2182.34 318.681 2187.11 318.681 Q2189.47 318.681 2191.55 319.029 Q2193.64 319.376 2195.39 320.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip4700)\" d=\"M 0 0 M2217.78 320.07 L2217.78 324.098 Q2215.97 323.172 2214.03 322.709 Q2212.08 322.246 2210 322.246 Q2206.83 322.246 2205.23 323.218 Q2203.66 324.191 2203.66 326.135 Q2203.66 327.616 2204.79 328.473 Q2205.93 329.306 2209.35 330.07 L2210.81 330.394 Q2215.35 331.366 2217.25 333.149 Q2219.17 334.908 2219.17 338.079 Q2219.17 341.69 2216.3 343.797 Q2213.45 345.903 2208.45 345.903 Q2206.37 345.903 2204.1 345.487 Q2201.85 345.093 2199.35 344.283 L2199.35 339.885 Q2201.71 341.112 2204.01 341.737 Q2206.3 342.339 2208.54 342.339 Q2211.55 342.339 2213.17 341.32 Q2214.79 340.278 2214.79 338.403 Q2214.79 336.667 2213.61 335.741 Q2212.46 334.816 2208.5 333.959 L2207.02 333.612 Q2203.06 332.778 2201.3 331.066 Q2199.54 329.329 2199.54 326.32 Q2199.54 322.663 2202.13 320.672 Q2204.72 318.681 2209.49 318.681 Q2211.85 318.681 2213.94 319.029 Q2216.02 319.376 2217.78 320.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(1:length(train_loss_results), train_loss_results, title=\"Loss\", label=\"Train loss\")\n",
    "plot!(1:length(test_loss_results), test_loss_results, label=\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
